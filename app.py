"""
EDC (Extract, Define, Canonicalize) - Streamlit Web UI
"""
import streamlit as st
import os
import sys
import tempfile
import json
from pathlib import Path
from dotenv import load_dotenv

# Load .env file
env_path = Path(__file__).parent / ".env"
if env_path.exists():
    load_dotenv(env_path)


def extract_text_from_pdf_azure_di(uploaded_file) -> list:
    """
    Azure Document Intelligence ã‚’ä½¿ç”¨ã—ã¦PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«æŠ½å‡º

    Args:
        uploaded_file: Streamlitã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    Returns:
        ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
    """
    from azure.core.credentials import AzureKeyCredential
    from azure.ai.documentintelligence import DocumentIntelligenceClient
    from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
    import re

    endpoint = os.environ.get("AZURE_DI_ENDPOINT")
    api_key = os.environ.get("AZURE_DI_API_KEY")
    model = os.environ.get("AZURE_DI_MODEL", "prebuilt-layout")

    if not endpoint or not api_key:
        raise ValueError("Azure Document Intelligence ã®è¨­å®šãŒå¿…è¦ã§ã™ï¼ˆAZURE_DI_ENDPOINT, AZURE_DI_API_KEYï¼‰")

    client = DocumentIntelligenceClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(api_key)
    )

    file_content = uploaded_file.read()
    poller = client.begin_analyze_document(
        model,
        AnalyzeDocumentRequest(bytes_source=file_content),
    )
    result = poller.result()

    # ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    texts = []
    if result.pages:
        for page in result.pages:
            page_num = page.page_number
            # ãƒšãƒ¼ã‚¸ç¯„å›²å†…ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            page_content = []
            if result.paragraphs:
                for para in result.paragraphs:
                    if hasattr(para, 'bounding_regions') and para.bounding_regions:
                        for region in para.bounding_regions:
                            if region.page_number == page_num:
                                page_content.append(para.content)
                                break

            if page_content:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                text = "\n".join(page_content)
                # æ—¥æœ¬èªã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
                text = re.sub(r'[ ]+([ã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾ ã€…ã€†ã€¤])', r'\1', text)
                text = re.sub(r'([ã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾ ã€…ã€†ã€¤])[ ]+', r'\1', text)
                texts.append(text.strip())

    # ãƒšãƒ¼ã‚¸ã”ã¨ã®æŠ½å‡ºãŒç©ºã®å ´åˆã€å…¨ä½“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½¿ç”¨
    if not texts and result.content:
        texts = [result.content]

    return texts


# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))
os.environ["TOKENIZERS_PARALLELISM"] = "false"

st.set_page_config(
    page_title="EDC - çŸ¥è­˜ãƒˆãƒªãƒ—ãƒ«æŠ½å‡º",
    page_icon="ğŸ”—",
    layout="wide"
)

st.title("ğŸ”— EDC: Extract, Define, Canonicalize")
st.markdown("LLMãƒ™ãƒ¼ã‚¹ã®çŸ¥è­˜ãƒˆãƒªãƒ—ãƒ«æŠ½å‡ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")

# Sidebar for configuration
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    # Check if Azure is configured in .env
    azure_configured = bool(
        os.environ.get("AZURE_OPENAI_ENDPOINT") and
        os.environ.get("AZURE_OPENAI_API_KEY")
    )

    # API Provider selection
    api_provider = st.selectbox(
        "API Provider",
        ["Azure OpenAI", "OpenAI"],
        index=0 if azure_configured else 1,
        help="ä½¿ç”¨ã™ã‚‹LLM APIã‚’é¸æŠ"
    )

    if api_provider == "Azure OpenAI":
        st.subheader("Azure OpenAI è¨­å®š")

        # Show status from .env
        if azure_configured:
            st.success("âœ… .envã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

        azure_endpoint = st.text_input(
            "Azure Endpoint",
            value=os.environ.get("AZURE_OPENAI_ENDPOINT", ""),
            type="default",
            help="ä¾‹: https://your-resource.openai.azure.com/"
        )
        azure_api_key = st.text_input(
            "Azure API Key",
            value=os.environ.get("AZURE_OPENAI_API_KEY", ""),
            type="password"
        )
        azure_api_version = st.text_input(
            "API Version",
            value=os.environ.get("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")
        )
        chat_deployment = st.text_input(
            "Chat Deployment Name",
            value=os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME", "gpt-4"),
            help="ãƒãƒ£ãƒƒãƒˆç”¨ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå"
        )
        embedding_deployment = st.text_input(
            "Embedding Deployment Name",
            value=os.environ.get("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME", "text-embedding-3-small"),
            help="åŸ‹ã‚è¾¼ã¿ç”¨ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå"
        )

        # Set environment variables
        if azure_endpoint:
            os.environ["AZURE_OPENAI_ENDPOINT"] = azure_endpoint
        if azure_api_key:
            os.environ["AZURE_OPENAI_API_KEY"] = azure_api_key
        if azure_api_version:
            os.environ["AZURE_OPENAI_API_VERSION"] = azure_api_version
        if chat_deployment:
            os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = chat_deployment
        if embedding_deployment:
            os.environ["AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME"] = embedding_deployment

        # Use "azure" to use .env settings
        llm_model = "azure"
        embedder_model = "azure"

    else:
        st.subheader("OpenAI è¨­å®š")
        openai_key = st.text_input(
            "OpenAI API Key",
            value=os.environ.get("OPENAI_KEY", ""),
            type="password"
        )
        if openai_key:
            os.environ["OPENAI_KEY"] = openai_key

        llm_model = st.selectbox(
            "Model",
            ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
            help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«"
        )

        # Embedder selection for OpenAI
        st.subheader("Embedder è¨­å®š")
        embedder_option = st.selectbox(
            "Sentence Embedder",
            [
                "all-MiniLM-L6-v2 (è»½é‡ãƒ»æ¨å¥¨)",
                "all-mpnet-base-v2 (ä¸­é‡)",
                "intfloat/e5-mistral-7b-instruct (é‡ã„)"
            ]
        )

        embedder_map = {
            "all-MiniLM-L6-v2 (è»½é‡ãƒ»æ¨å¥¨)": "all-MiniLM-L6-v2",
            "all-mpnet-base-v2 (ä¸­é‡)": "all-mpnet-base-v2",
            "intfloat/e5-mistral-7b-instruct (é‡ã„)": "intfloat/e5-mistral-7b-instruct"
        }
        embedder_model = embedder_map[embedder_option]

    st.divider()

    # Azure Document Intelligence settings
    st.subheader("ğŸ“„ PDFå‡¦ç†è¨­å®š")
    azure_di_configured = bool(
        os.environ.get("AZURE_DI_ENDPOINT") and
        os.environ.get("AZURE_DI_API_KEY")
    )

    if azure_di_configured:
        st.success("âœ… Azure DIè¨­å®šæ¸ˆã¿")
    else:
        st.warning("âš ï¸ Azure DIæœªè¨­å®šï¼ˆPDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸å¯ï¼‰")

    with st.expander("Azure Document Intelligence"):
        di_endpoint = st.text_input(
            "DI Endpoint",
            value=os.environ.get("AZURE_DI_ENDPOINT", ""),
            help="ä¾‹: https://your-resource.cognitiveservices.azure.com/"
        )
        di_api_key = st.text_input(
            "DI API Key",
            value=os.environ.get("AZURE_DI_API_KEY", ""),
            type="password"
        )
        di_model = st.selectbox(
            "DI Model",
            ["prebuilt-layout", "prebuilt-read", "prebuilt-document"],
            index=0,
            help="prebuilt-layoutæ¨å¥¨ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»å›³å¯¾å¿œï¼‰"
        )

        if di_endpoint:
            os.environ["AZURE_DI_ENDPOINT"] = di_endpoint
        if di_api_key:
            os.environ["AZURE_DI_API_KEY"] = di_api_key
        if di_model:
            os.environ["AZURE_DI_MODEL"] = di_model

    st.divider()

    # Advanced options
    with st.expander("è©³ç´°è¨­å®š"):
        enrich_schema = st.checkbox(
            "ã‚¹ã‚­ãƒ¼ãƒæ‹¡å¼µ",
            value=True,
            help="æ­£è¦åŒ–ã§ããªã„é–¢ä¿‚ã‚’æ–°ã—ã„ã‚¹ã‚­ãƒ¼ãƒã¨ã—ã¦è¿½åŠ ï¼ˆã‚¹ã‚­ãƒ¼ãƒãªã—ã§ä½¿ã†å ´åˆã¯å¿…é ˆï¼‰"
        )
        refinement_iterations = st.number_input(
            "åå¾©æ”¹å–„å›æ•°",
            min_value=0,
            max_value=5,
            value=0,
            help="Schema Retrieverã«ã‚ˆã‚‹åå¾©æ”¹å–„ï¼ˆ0=ãªã—ï¼‰"
        )

# Main content area
st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

uploaded_file = st.file_uploader(
    "ãƒ†ã‚­ã‚¹ãƒˆ/PDFãƒ•ã‚¡ã‚¤ãƒ«",
    type=["txt", "pdf"],
    help="ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¾ãŸã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆAzure DIã§å‡¦ç†ï¼‰"
)

# Schema options
st.subheader("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¹ã‚­ãƒ¼ãƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
use_schema = st.checkbox("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¹ã‚­ãƒ¼ãƒã‚’ä½¿ç”¨", value=False, help="ã‚¹ã‚­ãƒ¼ãƒãªã—ã§å®Ÿè¡Œã™ã‚‹ã¨ã‚¨ãƒƒã‚¸ã‚’è‡ªå‹•ç™ºè¦‹ã—ã¾ã™")

uploaded_schema = None
if use_schema:
    uploaded_schema = st.file_uploader(
        "ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.csvï¼‰",
        type=["csv"],
        help="relation,definitionå½¢å¼ã®CSVãƒ•ã‚¡ã‚¤ãƒ«"
    )

# Run button
st.divider()

if st.button("ğŸš€ ãƒˆãƒªãƒ—ãƒ«ã‚’æŠ½å‡º", type="primary", use_container_width=True):
    # Validate configuration
    if api_provider == "Azure OpenAI":
        if not os.environ.get("AZURE_OPENAI_ENDPOINT") or not os.environ.get("AZURE_OPENAI_API_KEY"):
            st.error("Azure OpenAIã®Endpointã¨API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„")
            st.stop()
    else:
        if not os.environ.get("OPENAI_KEY"):
            st.error("OpenAI API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„")
            st.stop()

    # Validate file upload
    if uploaded_file is None:
        st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        st.stop()

    # Prepare input
    if uploaded_file.name.endswith('.pdf'):
        # PDFå‡¦ç†ï¼ˆAzure Document Intelligenceï¼‰
        with st.spinner("PDFã‚’å‡¦ç†ä¸­... Azure Document Intelligence"):
            try:
                input_texts = extract_text_from_pdf_azure_di(uploaded_file)
                st.info(f"ğŸ“„ {len(input_texts)}ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
            except Exception as e:
                st.error(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                st.stop()
    else:
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        input_texts = uploaded_file.read().decode("utf-8").strip().split("\n")

    # Prepare schema
    schema_dict = {}
    if use_schema and uploaded_schema is not None:
        schema_content = uploaded_schema.read().decode("utf-8")
        for line in schema_content.strip().split("\n"):
            if "," in line:
                parts = line.split(",", 1)
                if len(parts) == 2:
                    schema_dict[parts[0].strip()] = parts[1].strip()

    # Run EDC
    with st.spinner("å‡¦ç†ä¸­... LLMã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™"):
        try:
            from edc.edc_framework import EDC
            import csv

            # Create temporary files
            with tempfile.TemporaryDirectory() as tmpdir:
                # Write schema to temp file
                schema_path = os.path.join(tmpdir, "schema.csv")
                with open(schema_path, "w", newline="", encoding="utf-8") as f:
                    writer = csv.writer(f)
                    for rel, defn in schema_dict.items():
                        writer.writerow([rel, defn])

                output_dir = os.path.join(tmpdir, "output")

                # EDC configuration
                edc_config = {
                    "oie_llm": llm_model,
                    "oie_prompt_template_file_path": "./prompt_templates/oie_template.txt",
                    "oie_few_shot_example_file_path": "./few_shot_examples/example/oie_few_shot_examples.txt",
                    "sd_llm": llm_model,
                    "sd_prompt_template_file_path": "./prompt_templates/sd_template.txt",
                    "sd_few_shot_example_file_path": "./few_shot_examples/example/sd_few_shot_examples.txt",
                    "sc_llm": llm_model,
                    "sc_embedder": embedder_model,
                    "sc_prompt_template_file_path": "./prompt_templates/sc_template.txt",
                    "sr_adapter_path": None,
                    "sr_embedder": embedder_model,
                    "oie_refine_prompt_template_file_path": "./prompt_templates/oie_r_template.txt",
                    "oie_refine_few_shot_example_file_path": "./few_shot_examples/example/oie_few_shot_refine_examples.txt",
                    "ee_llm": llm_model,
                    "ee_prompt_template_file_path": "./prompt_templates/ee_template.txt",
                    "ee_few_shot_example_file_path": "./few_shot_examples/example/ee_few_shot_examples.txt",
                    "em_prompt_template_file_path": "./prompt_templates/em_template.txt",
                    "target_schema_path": schema_path if schema_dict else None,
                    "enrich_schema": enrich_schema,
                    "loglevel": None,
                }

                # Change to edc directory for relative paths
                original_dir = os.getcwd()
                os.chdir(Path(__file__).parent)

                try:
                    edc = EDC(**edc_config)
                    results = edc.extract_kg(
                        input_texts,
                        output_dir,
                        refinement_iterations=refinement_iterations
                    )
                finally:
                    os.chdir(original_dir)

                # Display results
                st.success("æŠ½å‡ºå®Œäº†!")

                st.subheader("ğŸ“Š æŠ½å‡ºçµæœ")

                for idx, (text, triplets) in enumerate(zip(input_texts, results)):
                    with st.expander(f"ãƒ†ã‚­ã‚¹ãƒˆ {idx + 1}: {text[:50]}...", expanded=True):
                        st.markdown(f"**å…¥åŠ›:** {text}")
                        st.markdown("**æŠ½å‡ºã•ã‚ŒãŸãƒˆãƒªãƒ—ãƒ«:**")

                        if triplets:
                            # Create table
                            data = []
                            for t in triplets:
                                if t is not None and len(t) == 3:
                                    data.append({
                                        "Subject (ä¸»èª)": t[0],
                                        "Relation (é–¢ä¿‚)": t[1],
                                        "Object (ç›®çš„èª)": t[2]
                                    })

                            if data:
                                st.table(data)
                            else:
                                st.info("æ­£è¦åŒ–ã•ã‚ŒãŸãƒˆãƒªãƒ—ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
                        else:
                            st.info("ãƒˆãƒªãƒ—ãƒ«ã¯æŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

                # Edge summary (aggregate unique relations)
                st.subheader("ğŸ“ˆ ç™ºè¦‹ã•ã‚ŒãŸã‚¨ãƒƒã‚¸ï¼ˆã‚¹ã‚­ãƒ¼ãƒå€™è£œï¼‰")
                edge_summary = {}
                for triplets in results:
                    for t in triplets:
                        if t and len(t) == 3:
                            rel = t[1]
                            if rel not in edge_summary:
                                edge_summary[rel] = {"count": 0, "definition": "", "examples": []}
                            edge_summary[rel]["count"] += 1
                            if len(edge_summary[rel]["examples"]) < 3:
                                edge_summary[rel]["examples"].append((t[0], t[2]))

                # Get definitions from EDC schema
                for rel in edge_summary:
                    if rel in edc.schema:
                        edge_summary[rel]["definition"] = edc.schema[rel]

                if edge_summary:
                    edge_data = []
                    for rel, info in sorted(edge_summary.items(), key=lambda x: -x[1]["count"]):
                        examples_str = ", ".join([f"{s}â†’{o}" for s, o in info["examples"][:2]])
                        definition = info["definition"]
                        if len(definition) > 50:
                            definition = definition[:50] + "..."
                        edge_data.append({
                            "ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": rel,
                            "å®šç¾©": definition,
                            "å‡ºç¾å›æ•°": info["count"],
                            "ä¾‹": examples_str
                        })
                    st.table(edge_data)
                else:
                    st.info("ã‚¨ãƒƒã‚¸ã¯æŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

                # Export section
                st.subheader("ğŸ“¥ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")

                # JSON export
                export_data = []
                for idx, (text, triplets) in enumerate(zip(input_texts, results)):
                    export_data.append({
                        "input_text": text,
                        "triplets": [t for t in triplets if t is not None]
                    })

                col1, col2 = st.columns(2)
                with col1:
                    st.download_button(
                        label="JSONã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=json.dumps(export_data, indent=2, ensure_ascii=False),
                        file_name="triplets.json",
                        mime="application/json"
                    )

                # Schema CSV export
                with col2:
                    csv_lines = ["relation,definition,count"]
                    for rel, info in sorted(edge_summary.items(), key=lambda x: -x[1]["count"]):
                        definition = info["definition"].replace('"', '""')
                        csv_lines.append(f'"{rel}","{definition}",{info["count"]}')
                    csv_content = "\n".join(csv_lines)

                    st.download_button(
                        label="ã‚¹ã‚­ãƒ¼ãƒCSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv_content,
                        file_name="discovered_schema.csv",
                        mime="text/csv"
                    )

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

# Footer
st.divider()
st.markdown("""
---
**EDC Framework** - [GitHub](https://github.com/clear-nus/edc) |
è«–æ–‡: [Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction](https://arxiv.org/abs/2404.03868)
""")

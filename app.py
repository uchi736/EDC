"""
EDC (Extract, Define, Canonicalize) - Streamlit Web UI
"""
import streamlit as st
import os
import sys
import tempfile
import json
from pathlib import Path
from dotenv import load_dotenv

# Load .env file
env_path = Path(__file__).parent / ".env"
if env_path.exists():
    load_dotenv(env_path)


def extract_text_from_pdf_azure_di(uploaded_file) -> list:
    """
    Azure Document Intelligence ã‚’ä½¿ç”¨ã—ã¦PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒšãƒ¼ã‚¸ã”ã¨ã«æŠ½å‡º

    Args:
        uploaded_file: Streamlitã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ

    Returns:
        ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
    """
    from azure.core.credentials import AzureKeyCredential
    from azure.ai.documentintelligence import DocumentIntelligenceClient
    from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
    import re

    endpoint = os.environ.get("AZURE_DI_ENDPOINT")
    api_key = os.environ.get("AZURE_DI_API_KEY")
    model = os.environ.get("AZURE_DI_MODEL", "prebuilt-layout")

    if not endpoint or not api_key:
        raise ValueError("Azure Document Intelligence ã®è¨­å®šãŒå¿…è¦ã§ã™ï¼ˆAZURE_DI_ENDPOINT, AZURE_DI_API_KEYï¼‰")

    client = DocumentIntelligenceClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(api_key)
    )

    file_content = uploaded_file.read()
    poller = client.begin_analyze_document(
        model,
        AnalyzeDocumentRequest(bytes_source=file_content),
    )
    result = poller.result()

    # ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    texts = []
    if result.pages:
        for page in result.pages:
            page_num = page.page_number
            # ãƒšãƒ¼ã‚¸ç¯„å›²å†…ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            page_content = []
            if result.paragraphs:
                for para in result.paragraphs:
                    if hasattr(para, 'bounding_regions') and para.bounding_regions:
                        for region in para.bounding_regions:
                            if region.page_number == page_num:
                                page_content.append(para.content)
                                break

            if page_content:
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                text = "\n".join(page_content)
                # æ—¥æœ¬èªã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
                text = re.sub(r'[ ]+([ã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾ ã€…ã€†ã€¤])', r'\1', text)
                text = re.sub(r'([ã-ã‚“ã‚¡-ãƒ´ãƒ¼ä¸€-é¾ ã€…ã€†ã€¤])[ ]+', r'\1', text)
                texts.append(text.strip())

    # ãƒšãƒ¼ã‚¸ã”ã¨ã®æŠ½å‡ºãŒç©ºã®å ´åˆã€å…¨ä½“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½¿ç”¨
    if not texts and result.content:
        texts = [result.content]

    return texts


# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))
os.environ["TOKENIZERS_PARALLELISM"] = "false"

st.set_page_config(
    page_title="EDC - çŸ¥è­˜ãƒˆãƒªãƒ—ãƒ«æŠ½å‡º",
    page_icon="ğŸ”—",
    layout="wide"
)

st.title("ğŸ”— EDC: Extract, Define, Canonicalize")
st.markdown("LLMãƒ™ãƒ¼ã‚¹ã®çŸ¥è­˜ãƒˆãƒªãƒ—ãƒ«æŠ½å‡ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")

# Sidebar for configuration
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    # Check if Azure is configured in .env
    azure_configured = bool(
        os.environ.get("AZURE_OPENAI_ENDPOINT") and
        os.environ.get("AZURE_OPENAI_API_KEY")
    )

    # API Provider selection
    api_provider = st.selectbox(
        "API Provider",
        ["Azure OpenAI", "OpenAI"],
        index=0 if azure_configured else 1,
        help="ä½¿ç”¨ã™ã‚‹LLM APIã‚’é¸æŠ"
    )

    if api_provider == "Azure OpenAI":
        st.subheader("Azure OpenAI è¨­å®š")

        # Show status from .env
        if azure_configured:
            st.success("âœ… .envã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

        azure_endpoint = st.text_input(
            "Azure Endpoint",
            value=os.environ.get("AZURE_OPENAI_ENDPOINT", ""),
            type="default",
            help="ä¾‹: https://your-resource.openai.azure.com/"
        )
        azure_api_key = st.text_input(
            "Azure API Key",
            value=os.environ.get("AZURE_OPENAI_API_KEY", ""),
            type="password"
        )
        azure_api_version = st.text_input(
            "API Version",
            value=os.environ.get("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")
        )
        chat_deployment = st.text_input(
            "Chat Deployment Name",
            value=os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME", "gpt-4"),
            help="ãƒãƒ£ãƒƒãƒˆç”¨ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå"
        )
        embedding_deployment = st.text_input(
            "Embedding Deployment Name",
            value=os.environ.get("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME", "text-embedding-3-small"),
            help="åŸ‹ã‚è¾¼ã¿ç”¨ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå"
        )

        # Set environment variables
        if azure_endpoint:
            os.environ["AZURE_OPENAI_ENDPOINT"] = azure_endpoint
        if azure_api_key:
            os.environ["AZURE_OPENAI_API_KEY"] = azure_api_key
        if azure_api_version:
            os.environ["AZURE_OPENAI_API_VERSION"] = azure_api_version
        if chat_deployment:
            os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"] = chat_deployment
        if embedding_deployment:
            os.environ["AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME"] = embedding_deployment

        # Use "azure" to use .env settings
        llm_model = "azure"
        embedder_model = "azure"

    else:
        st.subheader("OpenAI è¨­å®š")
        openai_key = st.text_input(
            "OpenAI API Key",
            value=os.environ.get("OPENAI_KEY", ""),
            type="password"
        )
        if openai_key:
            os.environ["OPENAI_KEY"] = openai_key

        llm_model = st.selectbox(
            "Model",
            ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
            help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«"
        )

        # Embedder selection for OpenAI
        st.subheader("Embedder è¨­å®š")
        embedder_option = st.selectbox(
            "Sentence Embedder",
            [
                "all-MiniLM-L6-v2 (è»½é‡ãƒ»æ¨å¥¨)",
                "all-mpnet-base-v2 (ä¸­é‡)",
                "intfloat/e5-mistral-7b-instruct (é‡ã„)"
            ]
        )

        embedder_map = {
            "all-MiniLM-L6-v2 (è»½é‡ãƒ»æ¨å¥¨)": "all-MiniLM-L6-v2",
            "all-mpnet-base-v2 (ä¸­é‡)": "all-mpnet-base-v2",
            "intfloat/e5-mistral-7b-instruct (é‡ã„)": "intfloat/e5-mistral-7b-instruct"
        }
        embedder_model = embedder_map[embedder_option]

    st.divider()

    # Azure Document Intelligence settings
    st.subheader("ğŸ“„ PDFå‡¦ç†è¨­å®š")
    azure_di_configured = bool(
        os.environ.get("AZURE_DI_ENDPOINT") and
        os.environ.get("AZURE_DI_API_KEY")
    )

    if azure_di_configured:
        st.success("âœ… Azure DIè¨­å®šæ¸ˆã¿")
    else:
        st.warning("âš ï¸ Azure DIæœªè¨­å®šï¼ˆPDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸å¯ï¼‰")

    with st.expander("Azure Document Intelligence"):
        di_endpoint = st.text_input(
            "DI Endpoint",
            value=os.environ.get("AZURE_DI_ENDPOINT", ""),
            help="ä¾‹: https://your-resource.cognitiveservices.azure.com/"
        )
        di_api_key = st.text_input(
            "DI API Key",
            value=os.environ.get("AZURE_DI_API_KEY", ""),
            type="password"
        )
        di_model = st.selectbox(
            "DI Model",
            ["prebuilt-layout", "prebuilt-read", "prebuilt-document"],
            index=0,
            help="prebuilt-layoutæ¨å¥¨ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»å›³å¯¾å¿œï¼‰"
        )

        if di_endpoint:
            os.environ["AZURE_DI_ENDPOINT"] = di_endpoint
        if di_api_key:
            os.environ["AZURE_DI_API_KEY"] = di_api_key
        if di_model:
            os.environ["AZURE_DI_MODEL"] = di_model

    st.divider()

    # Advanced options
    with st.expander("è©³ç´°è¨­å®š"):
        enrich_schema = st.checkbox(
            "ã‚¹ã‚­ãƒ¼ãƒæ‹¡å¼µ",
            value=True,
            help="æ­£è¦åŒ–ã§ããªã„é–¢ä¿‚ã‚’æ–°ã—ã„ã‚¹ã‚­ãƒ¼ãƒã¨ã—ã¦è¿½åŠ ï¼ˆã‚¹ã‚­ãƒ¼ãƒãªã—ã§ä½¿ã†å ´åˆã¯å¿…é ˆï¼‰"
        )
        refinement_iterations = st.number_input(
            "åå¾©æ”¹å–„å›æ•°",
            min_value=0,
            max_value=5,
            value=0,
            help="Schema Retrieverã«ã‚ˆã‚‹åå¾©æ”¹å–„ï¼ˆ0=ãªã—ï¼‰"
        )

# Main content area
tab1, tab2, tab3 = st.tabs(["ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ğŸ“Š ã‚¹ã‚­ãƒ¼ãƒç®¡ç†"])

with tab1:
    st.subheader("ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çŸ¥è­˜ãƒˆãƒªãƒ—ãƒ«ã‚’æŠ½å‡º")

    input_text = st.text_area(
        "å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ",
        value="John Doe is a student at National University of Singapore.",
        height=150,
        help="çŸ¥è­˜ãƒˆãƒªãƒ—ãƒ«ã‚’æŠ½å‡ºã—ãŸã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›"
    )

    # Schema input
    st.subheader("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¹ã‚­ãƒ¼ãƒï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    use_schema = st.checkbox("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¹ã‚­ãƒ¼ãƒã‚’ä½¿ç”¨", value=True)

    if use_schema:
        default_schema = """student,The subject receives education at the institute specified by the object entity.
country,The subject entity is located in the country specified by the object entity.
place of birth,The subject entity was born in the location specified by the object entity.
date of birth,The subject entity was born on the date specified by the object entity.
occupation,The subject entity has the occupation specified by the object entity."""

        schema_text = st.text_area(
            "ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ï¼ˆCSVå½¢å¼: relation,definitionï¼‰",
            value=default_schema,
            height=150
        )

with tab2:
    st.subheader("ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿")

    uploaded_file = st.file_uploader(
        "ãƒ†ã‚­ã‚¹ãƒˆ/PDFãƒ•ã‚¡ã‚¤ãƒ«",
        type=["txt", "pdf"],
        help="ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã¾ãŸã¯PDFãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆAzure DIã§å‡¦ç†ï¼‰"
    )

    uploaded_schema = st.file_uploader(
        "ã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.csvï¼‰",
        type=["csv"],
        help="relation,definitionå½¢å¼ã®CSVãƒ•ã‚¡ã‚¤ãƒ«"
    )

with tab3:
    st.subheader("ã‚¹ã‚­ãƒ¼ãƒç®¡ç†")

    # Load existing schemas
    schema_dir = Path(__file__).parent / "schemas"
    if schema_dir.exists():
        schema_files = list(schema_dir.glob("*.csv"))
        if schema_files:
            selected_schema = st.selectbox(
                "æ—¢å­˜ã®ã‚¹ã‚­ãƒ¼ãƒã‚’é¸æŠ",
                [f.stem for f in schema_files]
            )

            if st.button("ã‚¹ã‚­ãƒ¼ãƒã‚’èª­ã¿è¾¼ã¿"):
                schema_path = schema_dir / f"{selected_schema}.csv"
                with open(schema_path, "r", encoding="utf-8") as f:
                    st.code(f.read(), language="csv")

# Run button
st.divider()

if st.button("ğŸš€ ãƒˆãƒªãƒ—ãƒ«ã‚’æŠ½å‡º", type="primary", use_container_width=True):
    # Validate configuration
    if api_provider == "Azure OpenAI":
        if not os.environ.get("AZURE_OPENAI_ENDPOINT") or not os.environ.get("AZURE_OPENAI_API_KEY"):
            st.error("Azure OpenAIã®Endpointã¨API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„")
            st.stop()
    else:
        if not os.environ.get("OPENAI_KEY"):
            st.error("OpenAI API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„")
            st.stop()

    # Prepare input
    if uploaded_file is not None:
        if uploaded_file.name.endswith('.pdf'):
            # PDFå‡¦ç†ï¼ˆAzure Document Intelligenceï¼‰
            with st.spinner("PDFã‚’å‡¦ç†ä¸­... Azure Document Intelligence"):
                try:
                    input_texts = extract_text_from_pdf_azure_di(uploaded_file)
                    st.info(f"ğŸ“„ {len(input_texts)}ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                except Exception as e:
                    st.error(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    st.stop()
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            input_texts = uploaded_file.read().decode("utf-8").strip().split("\n")
    else:
        input_texts = [input_text.strip()]

    # Prepare schema
    schema_dict = {}
    if use_schema:
        if uploaded_schema is not None:
            schema_content = uploaded_schema.read().decode("utf-8")
        else:
            schema_content = schema_text

        for line in schema_content.strip().split("\n"):
            if "," in line:
                parts = line.split(",", 1)
                if len(parts) == 2:
                    schema_dict[parts[0].strip()] = parts[1].strip()

    # Run EDC
    with st.spinner("å‡¦ç†ä¸­... LLMã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™"):
        try:
            from edc.edc_framework import EDC
            import csv

            # Create temporary files
            with tempfile.TemporaryDirectory() as tmpdir:
                # Write schema to temp file
                schema_path = os.path.join(tmpdir, "schema.csv")
                with open(schema_path, "w", newline="", encoding="utf-8") as f:
                    writer = csv.writer(f)
                    for rel, defn in schema_dict.items():
                        writer.writerow([rel, defn])

                output_dir = os.path.join(tmpdir, "output")

                # EDC configuration
                edc_config = {
                    "oie_llm": llm_model,
                    "oie_prompt_template_file_path": "./prompt_templates/oie_template.txt",
                    "oie_few_shot_example_file_path": "./few_shot_examples/example/oie_few_shot_examples.txt",
                    "sd_llm": llm_model,
                    "sd_prompt_template_file_path": "./prompt_templates/sd_template.txt",
                    "sd_few_shot_example_file_path": "./few_shot_examples/example/sd_few_shot_examples.txt",
                    "sc_llm": llm_model,
                    "sc_embedder": embedder_model,
                    "sc_prompt_template_file_path": "./prompt_templates/sc_template.txt",
                    "sr_adapter_path": None,
                    "sr_embedder": embedder_model,
                    "oie_refine_prompt_template_file_path": "./prompt_templates/oie_r_template.txt",
                    "oie_refine_few_shot_example_file_path": "./few_shot_examples/example/oie_few_shot_refine_examples.txt",
                    "ee_llm": llm_model,
                    "ee_prompt_template_file_path": "./prompt_templates/ee_template.txt",
                    "ee_few_shot_example_file_path": "./few_shot_examples/example/ee_few_shot_examples.txt",
                    "em_prompt_template_file_path": "./prompt_templates/em_template.txt",
                    "target_schema_path": schema_path if schema_dict else None,
                    "enrich_schema": enrich_schema,
                    "loglevel": None,
                }

                # Change to edc directory for relative paths
                original_dir = os.getcwd()
                os.chdir(Path(__file__).parent)

                try:
                    edc = EDC(**edc_config)
                    results = edc.extract_kg(
                        input_texts,
                        output_dir,
                        refinement_iterations=refinement_iterations
                    )
                finally:
                    os.chdir(original_dir)

                # Display results
                st.success("æŠ½å‡ºå®Œäº†!")

                st.subheader("ğŸ“Š æŠ½å‡ºçµæœ")

                for idx, (text, triplets) in enumerate(zip(input_texts, results)):
                    with st.expander(f"ãƒ†ã‚­ã‚¹ãƒˆ {idx + 1}: {text[:50]}...", expanded=True):
                        st.markdown(f"**å…¥åŠ›:** {text}")
                        st.markdown("**æŠ½å‡ºã•ã‚ŒãŸãƒˆãƒªãƒ—ãƒ«:**")

                        if triplets:
                            # Create table
                            data = []
                            for t in triplets:
                                if t is not None and len(t) == 3:
                                    data.append({
                                        "Subject (ä¸»èª)": t[0],
                                        "Relation (é–¢ä¿‚)": t[1],
                                        "Object (ç›®çš„èª)": t[2]
                                    })

                            if data:
                                st.table(data)
                            else:
                                st.info("æ­£è¦åŒ–ã•ã‚ŒãŸãƒˆãƒªãƒ—ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
                        else:
                            st.info("ãƒˆãƒªãƒ—ãƒ«ã¯æŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

                # JSON export
                st.subheader("ğŸ“¥ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
                export_data = []
                for idx, (text, triplets) in enumerate(zip(input_texts, results)):
                    export_data.append({
                        "input_text": text,
                        "triplets": [t for t in triplets if t is not None]
                    })

                st.download_button(
                    label="JSONã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=json.dumps(export_data, indent=2, ensure_ascii=False),
                    file_name="triplets.json",
                    mime="application/json"
                )

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

# Footer
st.divider()
st.markdown("""
---
**EDC Framework** - [GitHub](https://github.com/clear-nus/edc) |
è«–æ–‡: [Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction](https://arxiv.org/abs/2404.03868)
""")
